{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4178844f",
   "metadata": {},
   "source": [
    "# Calibrating the LDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afefc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from matplotlib.cm import ScalarMappable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8388facc",
   "metadata": {},
   "source": [
    "### 1.) Consider experimental nuclear masses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d600dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in the AME Data ##\n",
    "\n",
    "column_names = ['N','Z','BE/A']\n",
    "df = pd.read_csv('AME2020_BEperNucleon', delimiter=' ', names=column_names, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da4b106",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this cell is used for trying to tag certain elements ##\n",
    "\n",
    "tin_df = df[df['Z'] == 50]\n",
    "#print(tin_df)\n",
    "df132 = df[df['Z'] + df['N'] == 132]\n",
    "print(df132)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155f9217",
   "metadata": {},
   "source": [
    "### 2.) Take LDM mass expression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb64a15a",
   "metadata": {},
   "source": [
    "$$\n",
    "B = a_{vol}A - a_{surf}A^{2/3} - a_{sym}\\frac{(N-Z)^2}{A} - a_{c}\\frac{Z^2}{A^{1/3}}-\\delta (A)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bbc942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDM(N,Z,params):\n",
    "    A = N + Z\n",
    "    volterm = params[0]*A\n",
    "    surfterm = params[1]*A**(2./3.)\n",
    "    symterm = params[2]*((N-Z)**2)/A\n",
    "    coulterm = params[3]*((Z**2)/(A)**(1./3.))\n",
    "    if N%2 == 0 & Z%2 == 0:\n",
    "        pairterm = 0.\n",
    "    elif A%2 == 1:\n",
    "        pairterm = params[4]/(A**(1./2.))\n",
    "    else:\n",
    "        pairterm = 2*params[4]/(A**(1./2.))\n",
    "    Sum = volterm - surfterm - symterm - coulterm + pairterm\n",
    "    return Sum\n",
    "\n",
    "def chisquared(params):\n",
    "    num = len(df)\n",
    "    Chi2 = 0.0\n",
    "    for i in range(0,num):\n",
    "        line = df.iloc[i]\n",
    "        n = int(line['N'])\n",
    "        z = int(line['Z'])\n",
    "        a = n + z\n",
    "        be = float(line['BE/A'])*a\n",
    "        calc = LDM(n,z,params)\n",
    "        diff = be - calc\n",
    "        Chi2 = Chi2 + diff**2\n",
    "    return Chi2\n",
    "\n",
    "def LDM2(N,Z,params):\n",
    "    A = N + Z\n",
    "    volterm = params[0]*A\n",
    "    surfterm = params[1]*A**(2./3.)\n",
    "    symterm = params[2]*((N-Z)**2)/A\n",
    "    coulterm = params[3]*((Z**2)/(A)**(1./3.))\n",
    "    if N%2 == 0 & Z%2 == 0:\n",
    "        pairterm = 0.\n",
    "    elif A%2 == 1:\n",
    "        pairterm = params[4]/(A**(1./2.))\n",
    "    else:\n",
    "        pairterm = 2*params[4]/(A**(1./2.))\n",
    "    Sum = volterm - surfterm - symterm - coulterm + pairterm\n",
    "    return Sum/A\n",
    "\n",
    "def chisquared2(params):\n",
    "    num = len(df)\n",
    "    Chi2 = 0.0\n",
    "    for i in range(0,num):\n",
    "        line = df.iloc[i]\n",
    "        n = int(line['N'])\n",
    "        z = int(line['Z'])\n",
    "        a = n + z\n",
    "        be = float(line['BE/A'])\n",
    "        calc = LDM2(n,z,params)\n",
    "        diff = be - calc\n",
    "        Chi2 = Chi2 + diff**2\n",
    "    return Chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ff7b25",
   "metadata": {},
   "source": [
    "### 3.) Use chi-square approach to determine LDM parameters\n",
    "\n",
    "Below, I have an initial guess of the parameters based on the lecture slides. I then pass this initial guess to the scipy.optimize.minimize function with the BFGS optimization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95111886",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params = [15.68,18.56,28.1,0.717,0.]\n",
    "print(\"Initial parameters are:\")\n",
    "print(\"\")\n",
    "print(init_params)\n",
    "\n",
    "res = minimize(chisquared,init_params,method='BFGS')#bounds=bnds,method='BFGS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbd0752",
   "metadata": {},
   "source": [
    "After running the minimize function, below are the calibrated parameters I get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c9534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "print(\"volume term is \" + str(res.x[0]))\n",
    "print(\"surface term is \" + str(res.x[1]))\n",
    "print(\"symmetry term is \" + str(res.x[2]))\n",
    "print(\"coulomb term is \" + str(res.x[3]))\n",
    "print(\"pairing term is \" + str(res.x[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03d33af",
   "metadata": {},
   "source": [
    "Once we have our calibrated model, we can compare it with the data. Below is a graph showing the AME2020 data superimposed with the model based on the initial guess (blue) and the calibrated model (red). We can see it fits fairly well, but clearly not perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b2bbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['A'] = df['Z'] + df['N']\n",
    "#print(df)\n",
    "unique_a = df['A'].unique()\n",
    "#print(unique_a)\n",
    "unique_a_array = np.array([])\n",
    "max_be = np.array([])\n",
    "for unique_val in unique_a:\n",
    "    filtered_df = df[df['A'] == unique_val]\n",
    "    max_col_row = filtered_df.loc[filtered_df['BE/A'].idxmax()]\n",
    "    unique_a_array = np.append(unique_a_array,unique_val)\n",
    "    max_be = np.append(max_be,max_col_row['BE/A'])\n",
    "\n",
    "newParams = res.x\n",
    "\n",
    "new_df = df[(df['A'].isin(unique_a_array)) & (df['BE/A'].isin(max_be))]\n",
    "new_df = new_df.sort_values(by='A')\n",
    "\n",
    "a_val = []\n",
    "be_val1 = []\n",
    "be_val2 = []\n",
    "for i in range(0,len(new_df)):\n",
    "    n = int(new_df.iloc[i]['N'])\n",
    "    z = int(new_df.iloc[i]['Z'])\n",
    "    a = n + z\n",
    "    a_val.append(new_df.iloc[i]['A'])\n",
    "    be_val1.append(LDM2(n,z,init_params))\n",
    "    be_val2.append(LDM2(n,z,newParams))\n",
    "\n",
    "plt.figure(1)\n",
    "plt.title(\"Binding Energy per Nucleon\")\n",
    "plt.ylabel(\"B/A (MeV)\")\n",
    "plt.xlabel(\"Mass Number (A)\")\n",
    "plt.scatter(unique_a_array,max_be,color='black',marker='o',s=5,label=\"AME2020 Data\")\n",
    "plt.plot(a_val,be_val1,'b',label=\"Initial Guess\")\n",
    "plt.plot(a_val,be_val2,'r',label=\"Calibrated LDM\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e1e26b",
   "metadata": {},
   "source": [
    "To investigate the discrepancies from the model, I have plotted the residuals of each data point from the model in three manners. 1) In the form of the chart of the nuclides, where we can see an almost staircase-like pattern emerging. Note, as the model does not work as well for lower mass nuclei, if we exclude the lightest and most exotic nuclei, this pattern emerges even clearer. The other two forms are residuals vs 2) proton number and 3) neutron number. In these last two depictions of the residuals, the peaks are consistent at numbers 28, 50, and 82, whereas the neutron plot includes a peak near 126!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02efa281",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this cell will create a new column to keep track of residuals ##\n",
    "\n",
    "resid = []\n",
    "Narr = []\n",
    "Zarr = []\n",
    "for i in range(0,len(df)):\n",
    "    line = df.iloc[i]\n",
    "    n = int(line['N'])\n",
    "    z = int(line['Z'])\n",
    "    bea = float(line['BE/A'])\n",
    "    becalc = LDM(n,z,res.x)/(n+z)\n",
    "    resid.append((bea-becalc))\n",
    "    Narr.append(n)\n",
    "    Zarr.append(z)\n",
    "df['resid'] = resid\n",
    "\n",
    "plt.figure(2)\n",
    "plt.scatter(Narr,Zarr,c=resid,marker='s',cmap='viridis',s=3)\n",
    "plt.xlabel(\"Neutrons\")\n",
    "plt.ylabel(\"Protons\")\n",
    "plt.title(\"Residuals from LDM on Chart of Nuclides\")\n",
    "plt.colorbar(label=\"Scale\")\n",
    "\n",
    "plt.figure(3)\n",
    "plt.title(\"Residuals from LDM vs Protons\")\n",
    "plt.xlabel(\"Protons\")\n",
    "plt.ylabel(\"BE/A\")\n",
    "plt.plot(Zarr,df['resid'],'b.',markersize=2)\n",
    "\n",
    "plt.figure(4)\n",
    "plt.title(\"Residuals from LDM vs Neutrons\")\n",
    "plt.xlabel(\"Neutrons\")\n",
    "plt.ylabel(\"BE/A\")\n",
    "plt.plot(Narr,df['resid'],'b.',markersize=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f17370",
   "metadata": {},
   "source": [
    "### 4.) Compute the Hessian and covariance matrices\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c602c0a",
   "metadata": {},
   "source": [
    "To compute the Hessian, we must compute the second partial derivatives of the $\\chi^{2}$ function with respect to each of the five input parameters. Thus, we construct a 5x5 matrix where each element is the summation across each data point. Conveniently, the BFGS method in scipy.optimize.minimize computes an inverse Hessian that we can take the inverse of to find our Hessian. Below is the printed Hessian from the optimization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fbb16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hessian from BFGS:\")\n",
    "print(\"\")\n",
    "print(np.linalg.inv(res.hess_inv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39f90b8",
   "metadata": {},
   "source": [
    "However, in the spirit of pursuing knowledge, I analytically computed the Hessian with the expression for our $\\chi^{2}$ and it is printed below. The Hessian's match up very well with the exception of the elements relating to our pairing term parameter. However, the discrepancy is consistent across some constant value, so at least it's consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d612bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipyhess = np.linalg.inv(res.hess_inv)\n",
    "Hess = np.zeros((5,5))\n",
    "for i in range(0,len(df)):\n",
    "    line = df.iloc[i]\n",
    "    n = int(line['N'])\n",
    "    z = int(line['Z'])\n",
    "    a = n + z\n",
    "    if n%2 == 0 and z%2 == 0:\n",
    "        pair=0.\n",
    "    elif a%2 == 0:\n",
    "        pair = -2./np.sqrt(a)\n",
    "    else:\n",
    "        pair = -1./np.sqrt(a)\n",
    "    Hess[0,0] = Hess[0,0] + a**2\n",
    "    Hess[0,1] = Hess[0,1] - a**(5./3)\n",
    "    Hess[1,1] = Hess[1,1] + a**(4./3)\n",
    "    Hess[2,2] = Hess[2,2] + (n-z)**(4)/a**2\n",
    "    Hess[3,3] = Hess[3,3] + z**4/a**(2/3)\n",
    "    Hess[4,4] = Hess[4,4] + pair**2\n",
    "    Hess[0,2] = Hess[0,2] - (n-z)**2\n",
    "    Hess[0,3] = Hess[0,3] - (z**2)*a**(2/3)\n",
    "    Hess[0,4] = Hess[0,4] - pair*a\n",
    "    Hess[1,2] = Hess[1,2] + (n-z)**2/a**(1/3)\n",
    "    Hess[1,3] = Hess[1,3] + z**2*a**(1/3)\n",
    "    Hess[1,4] = Hess[1,4] + pair*a**(2/3)\n",
    "    Hess[2,3] = Hess[2,3] + z**2*(n-z)**2/a**(4/3)\n",
    "    Hess[2,4] = Hess[2,4] + pair*(n-z)**2/a\n",
    "    Hess[3,4] = Hess[3,4] + pair*z**2/a**(1/3)\n",
    "Hess[1,0] = Hess[0,1]\n",
    "Hess[2,0] = Hess[0,2]\n",
    "Hess[2,1] = Hess[1,2]\n",
    "Hess[3,0] = Hess[0,3]\n",
    "Hess[3,1] = Hess[1,3]\n",
    "Hess[3,2] = Hess[2,3]\n",
    "Hess[4,0] = Hess[0,4]\n",
    "Hess[4,1] = Hess[1,4]\n",
    "Hess[4,2] = Hess[2,4]\n",
    "Hess[4,3] = Hess[3,4]\n",
    "\n",
    "Hess = 2*Hess\n",
    "print(\"Below is my computed Hessian:\")\n",
    "print(\"\")\n",
    "print(Hess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace9b62e",
   "metadata": {},
   "source": [
    "Next, we can find the covariance matrix by multiplying our inverse Hessian with a scale factor, s, designed to normalize the $\\chi^{2}$ function to the number of data points present minus number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a8c064",
   "metadata": {},
   "outputs": [],
   "source": [
    "chiinit = chisquared(init_params)\n",
    "Nd = len(df)\n",
    "Np = len(init_params)\n",
    "s = (chiinit)/(Nd-Np)\n",
    "print(\"The chi-squared of our initial parameters:\")\n",
    "print(chiinit)\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"S-factor (not the astrophysical one...):\")\n",
    "print(s)\n",
    "\n",
    "cov_mat = s*res.hess_inv\n",
    "print(\"\")\n",
    "print(\"Covariance matrix:\")\n",
    "print(\"\")\n",
    "print(cov_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a45287",
   "metadata": {},
   "source": [
    "### 5.) Compute the matrix of correlations between LDM parameters\n",
    "\n",
    "From here, we have the variance of each parameter with itself and with other variables. Thus, we can find the correlations between each variable to provide additional insight into the model parameters. As shown below, the correlation between each variable is quite strong with the exception of the pairing term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fdfe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = np.zeros((5,5))\n",
    "\n",
    "for i in range(0,5):\n",
    "    for j in range(0,5):\n",
    "        corr_mat[i,j] = cov_mat[i,j]/(np.sqrt(cov_mat[i,i])*np.sqrt(cov_mat[j,j]))\n",
    "\n",
    "print(\"Correlation matrix:\")\n",
    "print(\"\")\n",
    "print(corr_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d3727c",
   "metadata": {},
   "source": [
    "### 6.) What is the effective number of LDM parameters?\n",
    "\n",
    "To do this, we first scale the Hessian to a conditioned Hessian, which essentially follows the same process as going from covariance to correlation. From here, we can perform a singular value decomposition on the conditioned Hessian and get the eigenvalues associated with each parameter. From here, I then normalize the eigenvalues to where the sum equals 1. We can then see that the first term, the volume parameter, dominates over the other variables. If we define our number of effective parameters to be those whose eigenvalues add to (N-1)/N, where N is the number of parameters, then we can see that there is only one effective parameter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c50776",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_hess = np.zeros((5,5))\n",
    "\n",
    "for i in range(0,5):\n",
    "    for j in range(0,5):\n",
    "        cond_hess[i,j] = Hess[i,j]/(np.sqrt(Hess[i,i])*np.sqrt(Hess[j,j]))\n",
    "\n",
    "SVD = np.linalg.svd(cond_hess)\n",
    "weight = sum(SVD[1])\n",
    "print(\"Eigenvalues:\")\n",
    "print(\"\")\n",
    "print(SVD[1]/weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06732a18",
   "metadata": {},
   "source": [
    "### 7.) Will the result be the same if the model is calibrated to the binding energy per nucleon?\n",
    "\n",
    "Yes and no. The fit parameters will give a similar result, but the Hessian, covariance, and correlation matrices are not the same. In this case, the Hessian is divided by a factor of A$^2$, so it is considerably smaller. Additionally, this model will allow the deviation in the higher mass region to not affect the fit as much since binding energy increases with A. The biggest difference I notice here, is how the variables correlate differently and the fact that there are now two parameters with dominating eigenvalues. In this case, it is both the volume term and the surface term that are required to achieve the confidence condition defined in part 6. Thus, there are now two effective parameters for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae652536",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = minimize(chisquared2,init_params,method='BFGS')#bounds=bnds,method='BFGS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eec9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The LDM parameters using BE/A:\")\n",
    "print(\"\")\n",
    "print(\"volume term is \" + str(res2.x[0]))\n",
    "print(\"surface term is \" + str(res2.x[1]))\n",
    "print(\"symmetry term is \" + str(res2.x[2]))\n",
    "print(\"coulomb term is \" + str(res2.x[3]))\n",
    "print(\"pairing term is \" + str(res2.x[4]))\n",
    "\n",
    "scipyhess2 = np.linalg.inv(res2.hess_inv)\n",
    "Hess2 = np.zeros((5,5))\n",
    "for i in range(0,len(df)):\n",
    "    line = df.iloc[i]\n",
    "    n = int(line['N'])\n",
    "    z = int(line['Z'])\n",
    "    a = n + z\n",
    "    if n%2 == 0 and z%2 == 0:\n",
    "        pair=0.\n",
    "    elif a%2 == 0:\n",
    "        pair = -2./np.sqrt(a**3)\n",
    "    else:\n",
    "        pair = -1./np.sqrt(a**3)\n",
    "    Hess2[0,0] = Hess2[0,0] + 1\n",
    "    Hess2[0,1] = Hess2[0,1] - a**(-1./3)\n",
    "    Hess2[1,1] = Hess2[1,1] + a**(-2./3)\n",
    "    Hess2[2,2] = Hess2[2,2] + (n-z)**(4)/a**4\n",
    "    Hess2[3,3] = Hess2[3,3] + z**4/a**(8/3)\n",
    "    Hess2[4,4] = Hess2[4,4] + pair**2\n",
    "    Hess2[0,2] = Hess2[0,2] - (n-z)**2/a**2\n",
    "    Hess2[0,3] = Hess2[0,3] - (z**2)*a**(-4/3)\n",
    "    Hess2[0,4] = Hess2[0,4] - pair\n",
    "    Hess2[1,2] = Hess2[1,2] + (n-z)**2/a**(7/3)\n",
    "    Hess2[1,3] = Hess2[1,3] + z**2*a**(-5/3)\n",
    "    Hess2[1,4] = Hess2[1,4] + pair*a**(-1/3)\n",
    "    Hess2[2,3] = Hess2[2,3] + z**2*(n-z)**2/a**(10/3)\n",
    "    Hess2[2,4] = Hess2[2,4] + pair*(n-z)**2/a**2\n",
    "    Hess2[3,4] = Hess2[3,4] + pair*z**2/a**(-5/3)\n",
    "Hess2[1,0] = Hess2[0,1]\n",
    "Hess2[2,0] = Hess2[0,2]\n",
    "Hess2[2,1] = Hess2[1,2]\n",
    "Hess2[3,0] = Hess2[0,3]\n",
    "Hess2[3,1] = Hess2[1,3]\n",
    "Hess2[3,2] = Hess2[2,3]\n",
    "Hess2[4,0] = Hess2[0,4]\n",
    "Hess2[4,1] = Hess2[1,4]\n",
    "Hess2[4,2] = Hess2[2,4]\n",
    "Hess2[4,3] = Hess2[3,4]\n",
    "\n",
    "Hess2 = 2*Hess2\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"Hessian is below:\")\n",
    "print(\"\")\n",
    "print(Hess2)\n",
    "\n",
    "chiinit2 = chisquared2(init_params)\n",
    "Nd = len(df)\n",
    "Np = len(init_params)\n",
    "s2 = (chiinit2)/(Nd-Np)\n",
    "cov_mat2 = s2*res2.hess_inv\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"Covariance matrix is below:\")\n",
    "print(\"\")\n",
    "print(cov_mat2)\n",
    "corr_mat2 = np.zeros((5,5))\n",
    "\n",
    "for i in range(0,5):\n",
    "    for j in range(0,5):\n",
    "        corr_mat2[i,j] = cov_mat2[i,j]/(np.sqrt(cov_mat2[i,i])*np.sqrt(cov_mat2[j,j]))\n",
    "\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"Correlation matrix is below:\")\n",
    "print(\"\")\n",
    "print(corr_mat2)\n",
    "\n",
    "cond_hess2 = np.zeros((5,5))\n",
    "\n",
    "for i in range(0,5):\n",
    "    for j in range(0,5):\n",
    "        cond_hess2[i,j] = Hess2[i,j]/(np.sqrt(Hess2[i,i])*np.sqrt(Hess2[j,j]))\n",
    "\n",
    "SVD2 = np.linalg.svd(cond_hess2)\n",
    "weight2 = sum(SVD2[1])\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"Eigenvalues:\")\n",
    "print(\"\")\n",
    "print(SVD2[1]/weight2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c40f019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132b5b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54edac95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0144d159",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(LDM(90,73,newParams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267567ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "8.027171-8.014767"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3136a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "7.9818905-7.975495"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f700c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['resid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6045cec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
